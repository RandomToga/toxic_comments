{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9abe47-5f55-4f93-95f2-28c8cd548f9f",
   "metadata": {},
   "source": [
    "Ситуация: на платформе появилось много плохих комментариев. От продакт-менеджера поступила задача - избавиться от плохих комментариев.\n",
    "Переход от продуктовой задачи к ML задаче:\n",
    "Разработать бинарный классификатор, который на вход будет принимать текст комментария, на выходе выдавать 2 класса - хороший/плохой комментарий.\n",
    "Метрики и ограничения:\n",
    "Задача - максимизировать количество удаленных негативных комментариев. В идеальном случае мы должны удалять их все, но мы не хотим удалять хорошие комментарии,\n",
    "чтобы пользователи не обижались\n",
    "Ограничение: в 1 из 20 случаев мы имеем право ошибиться, то есть удалить хороший комментарий вместо плохого.\n",
    "2 метрики:\n",
    "recall -  соотношение найденных плохих комментариев\n",
    "(если эта метрика = 1, значит мы нашли всеплохие комментарии)\n",
    "precision - вероятность того, что если мы сказали, что комментарий является реально плохим, то он им реально является\n",
    "(поставим precision >0.95)\n",
    "В итоге задача:\n",
    "> Разработать бинарный классификатор\n",
    "> Максимизировать recall\n",
    "> precision > 0.95\n",
    "Следующий этап: Составление датасета\n",
    "Можно разметить данные самому, но это очень дорого, поэтому попробуем найти готовый датасет.\n",
    "Прям так и пишем в гугл: \"датасет токсичных комментариев\"\n",
    "находим https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "240161a7-1400-4ddd-b1c2-584867831c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')  # Скачиваем стоп-слова\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bfd93-88f2-4a4a-8eaf-5d84f1e65f6e",
   "metadata": {},
   "source": [
    "## Анализ данных (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62efc1c-a745-4f56-959c-71406ecad37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/labeled.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5470db-5c24-4204-a264-b9842a3f6553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14412, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f16c464-5b8e-4ee0-83df-7d97a40904c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b027b2-bf1b-41d7-9e73-8d9a42e2ef1c",
   "metadata": {},
   "source": [
    "видим, что в столбце toxic данные типа float. переведем их в тип int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc1fd6a-9766-491e-8164-142468dccaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"toxic\"] = df[\"toxic\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee61ff0b-9ea4-4ef5-b8ab-4088899a2b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n      1\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...      1\n",
       "2                          Собаке - собачья смерть\\n      1\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...      1\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1d7ca-00d5-425a-9e78-8df2b47372d3",
   "metadata": {},
   "source": [
    "посмотрим распределение данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ebedf72-48f6-44a0-b797-1709178d7d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    9586\n",
       "1    4826\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12488cac-c28e-4b17-827e-303eb63a336f",
   "metadata": {},
   "source": [
    "посмотрим, что подразумевается под токсичными комментариями в данном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfae0e88-7b77-4a44-ba5c-fdb42cf508dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Верблюдов-то за что? Дебилы, бл...\n",
      "\n",
      "Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\n",
      "\n",
      "Собаке - собачья смерть\n",
      "\n",
      "Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\n",
      "\n",
      "тебя не убедил 6-страничный пдф в том, что Скрипалей отравила Россия? Анализировать и думать пытаешься? Ватник что ли?)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in df[df[\"toxic\"]==1][\"comment\"].head(5):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed298c8f-36b0-44a9-bad3-f44988528925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В шапке были ссылки на инфу по текущему фильму марвел. Эти ссылки были заменены на фразу Репортим брипидора, игнорируем его посты. Если этого недостаточно, чтобы понять, что модератор абсолютный неадекват, и его нужно лишить полномочий, тогда эта борда пробивает абсолютное дно по неадекватности.\n",
      "\n",
      "Почитайте посты у этого автора,может найдете что нибудь полезное. Надеюсь помог) https: pikabu.ru story obyichnyie budni dezsluzhbyi 4932098\n",
      "\n",
      "Про графику было обидно) я так то проходил все серии гта со второй части по пятую, кроме гта 4. И мне не мешала графика ни в одной из частей. На компе у меня было куча видеокарт. Начиная с 32мб RIVA TNT и заканчивая 2Гб 560Ti на которой я спокойно играю который год в танки, гта5, ведьмака3 купил на распродаже и начал проходить. Да, не на ультрах. С пониженными текстурами. И не мешает. Я не понимаю дрочева на графике, требовать графику уровня плойки 4 минимум. Мне надо чтобы глаза не резало, только и всего. По поводу управления, мне не хватает переходника на type c. У меня джойстик есть от иксбокса360. Потенциала в мобильных играх достаточно чтобы забить кнопки как забивались в той же NFS MW в 2005. Не самая плохая игра была.\n",
      "\n",
      "https: pp.userapi.com c848520 v848520411 11627b cOhWqFbGjWE.jpg\n",
      "\n",
      "Возьмём как пример Россию, западноевропейские страны и США. Идёт метисация, сознательная политика замещения белого населения на пришлое черно-коричневое. Идёт создание новой расы метисов, исламизация и почернение. В крупных городах половина населения - выходцы из ебеней Мексики, Африки, Ближнего Востока, а в случае с Россией - Кавказа и Средней Азии. Этнические ниггеро-арабские гетто верят на хую законы как хотят, чудовищная по масштабам этническая преступность. Говорить о миграции и тем более затрагивать тему замещения коренного населения властями нельзя, иначе бутылка. Свобода слова тут не для вас, молодой человек. При этом говорить о том, что белые должны вымереть, и это нормально - можно. Белые официально вымирают ведётся пропаганда так или иначе направленная на снижение рождаемости белого населения. Феминизм, ЛГБТ, чайлдфри. Каждая женщина в Швеции - леволиберальная феминистка, это страна победившего феминизма. Что сегодня там происходит - страшно делается. Пропагандируются смешанные браки, межрасовые браки, пропагандируется превосходство детей-метисов. Идёт демонизация белых и пропаганда превосходства чёрных и смуглых мужчин, форс отношений белая женщина смуглый чёрный мужчина-мигрант. Как результат - всё больше чернильниц, всё больше смешанных браков, всё больше небелых метисов. Белые женщины просто не хотят контактировать с мужчинами своей нации и расы, наделяя их самыми плохими качествами и обожествляя черных. При этом большинство белых не считает завоз чурок чем-то плохим, наоборот, относятся к ним толерантно. Проводится политика насаждения толерантности, мультикультурализма, политкорректности и космополитизма. Набирающее популярность даже в России SJW - это вообще отдельная тема для обсуждения. Всё вышеперечисленное относится к сильнейшим когда-то странам, бывшим империям, нагибающим слабых. Сегодня происходит так, что бывшие империи в прямом смысле деградируют, вырождаются и вымирают, а место сильнейших когда-то, господствующих народов, занимают те, кого когда-то колонизировали. Во Франции к 2080 уже будут доминировать негры и арабы, в России - кавказцы и выходцы из средней Азии, в Великобритании - индийцы, негры, арабы, пакистанцы, etc. А в маленьких, нейтральных странах, вроде Словении или Беларуси, Литвы или Чехии, Румынии или Эстонии - всё пучком. Им вымирание не грозит, они остаются и будут оставаться белыми. Более того, у них ведётся политика, направленная на сохранение традиционных ценностей и культуры коренного населения. Они сказали беженцам нет . В Польшу, например, русскому или украинцу гораздо легче переехать и остаться, чем арабу или африканцу. В Германии ситуация противоположная, белых там не ждут. Польша, Чехия, Словакия, Венгрия, Словения, Хорватия, Сербия, БиГ, Черногория, Македония, Греция, Болгария, Румыния, Молдова, Украина, Беларусь, Литва, Латвия, Эстония - вот Европа будущего. Скандинавия, Южная, Западная Европа, а также Россия - лишатся коренного населения и своей культуры.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in df[df[\"toxic\"]==0][\"comment\"].head(5):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b6056-cd4a-461f-9570-d48670159cb8",
   "metadata": {},
   "source": [
    "видим, что разметка данных не очень хорошая, так как даже в положительные комментарии попали достаточно негативные высказывания. но работать будем с тем, что имеется."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77190b-5398-42ae-8568-6cd51801d7dc",
   "metadata": {},
   "source": [
    "В реальности для тестового датасета нужно использовать данные с таким же распределением, что и в продакшн системе, но у нас нет на самом деле никакго сервиса, поэтому возьмем 500 сэймплов из нашего датасета как тестовые и предположим, что распределение в них совпадает с нашей придуманной системой.\n",
    "используем функцию train_test_split из библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f9948e-180d-4ced-8587-4ee68d6eebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e1e21-7edd-43fa-94dc-db2801af4e15",
   "metadata": {},
   "source": [
    "проверим, что у нас действительно 500 samples на тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c39b118-9c75-4c7c-bf57-a9dc94b8fb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86218471-8343-466d-b567-add9149b115e",
   "metadata": {},
   "source": [
    "проверим распределение в test и в train, удостоверимся, что оно примерно одинаковое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d7ebb1-3f86-4272-8e04-24d991fad28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    312\n",
       "1    188\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a6d5231-9760-460b-8ad8-0ad660ca3362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    9274\n",
       "1    4638\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea9e54-d01c-4735-85b6-02b2050dcb9f",
   "metadata": {},
   "source": [
    "и там и там распределение 1 к 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b65773-e0cf-42af-898e-1da3f238ac6a",
   "metadata": {},
   "source": [
    "Буду испозльзовать самую простую модель - логистическую регрессию из библиотеки scikit-learn.  \n",
    "Перед тем как подавать данные на вход, надо из текста сделать численные векторы, потому что сама модель принимает на вход вещественные векторы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8b54a-7692-4a33-b6c0-074266acd7c0",
   "metadata": {},
   "source": [
    "## Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974164ee-ff93-40da-8511-afd5af3ad203",
   "metadata": {},
   "source": [
    "Этапы:  \n",
    "- Разбить текст на токены\n",
    "- Удалить токены, не несущие смысловой информации (знаки пунктуации, стоп слова)  \n",
    "- К каждому токену применить стемминг (stemming)(удалить окончания, привести все к нижнему регистру)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c4ec5c3-f6c0-4926-bfd6-7a973dbd1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find('tokenizers/punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef91cdce-86d1-4a8f-84d9-50022a1dd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_example = df.iloc[1][\"comment\"]\n",
    "\n",
    "tokens = word_tokenize(sentence_example, language=\"russian\")\n",
    "tokens_without_punctuation = [i for i in tokens if i not in string.punctuation]\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "tokens_without_stop_words_and_punctuation = [i for i in tokens_without_punctuation if i not in russian_stop_words]\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "\n",
    "stemmed_tokens = [snowball.stem(i) for i in tokens_without_stop_words_and_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21937c4c-27ac-4fb7-b8dc-1d9188ffd18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\n",
      "\n",
      "--------------------\n",
      "Токены: ['Хохлы', ',', 'это', 'отдушина', 'затюканого', 'россиянина', ',', 'мол', ',', 'вон', ',', 'а', 'у', 'хохлов', 'еще', 'хуже', '.', 'Если', 'бы', 'хохлов', 'не', 'было', ',', 'кисель', 'их', 'бы', 'придумал', '.']\n",
      "--------------------\n",
      "Токены без пунктуации: ['Хохлы', 'это', 'отдушина', 'затюканого', 'россиянина', 'мол', 'вон', 'а', 'у', 'хохлов', 'еще', 'хуже', 'Если', 'бы', 'хохлов', 'не', 'было', 'кисель', 'их', 'бы', 'придумал']\n",
      "--------------------\n",
      "Токены без пунктуации и стоп-слов: ['Хохлы', 'это', 'отдушина', 'затюканого', 'россиянина', 'мол', 'вон', 'хохлов', 'хуже', 'Если', 'хохлов', 'кисель', 'придумал']\n",
      "--------------------\n",
      "Токены после стемминга: ['хохл', 'эт', 'отдушин', 'затюкан', 'россиянин', 'мол', 'вон', 'хохл', 'хуж', 'есл', 'хохл', 'кисел', 'придума']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Исходный текст: {sentence_example}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"Токены: {tokens}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"Токены без пунктуации: {tokens_without_punctuation}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"Токены без пунктуации и стоп-слов: {tokens_without_stop_words_and_punctuation}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"Токены после стемминга: {stemmed_tokens}\")\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c680e8-5793-447e-bd71-b3f16193fd9f",
   "metadata": {},
   "source": [
    "напишем функцию, чтобы обработать все значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf4e72cb-5a52-41dc-9807-0064445fc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "\n",
    "def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\n",
    "    tokens = word_tokenize(sentence, language=\"russian\")\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in russian_stop_words]\n",
    "    tokens = [snowball.stem(i) for i in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42687f-7d9e-4ee4-9105-a090c01ef66c",
   "metadata": {},
   "source": [
    "проверим ее на примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0de6ca4e-b313-4ce5-87a3-ee59a7db20ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['хохл',\n",
       " 'эт',\n",
       " 'отдушин',\n",
       " 'затюкан',\n",
       " 'россиянин',\n",
       " 'мол',\n",
       " 'вон',\n",
       " 'хохл',\n",
       " 'хуж',\n",
       " 'есл',\n",
       " 'хохл',\n",
       " 'кисел',\n",
       " 'придума']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence(sentence_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34388035-c898-46ff-9edd-ed15494d9b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
